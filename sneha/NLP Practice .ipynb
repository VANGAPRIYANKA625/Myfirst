{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd04e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d7579fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Narendra', 'Modi', 'is', 'the', 'Prime', 'Minister', 'of', 'India', '.', 'Barracck', 'Obamo', 'is', 'the', 'president', 'of', 'USA']\n",
      "16 \n",
      "\n",
      "['Narendra Modi is the Prime Minister of India.', 'Barracck Obamo is the president of USA']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = \"Narendra Modi is the Prime Minister of India. Barracck Obamo is the president of USA\"\n",
    "word_tokens = word_tokenize(text)\n",
    "print(word_tokens)\n",
    "print(len(word_tokens), \"\\n\")\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)\n",
    "print(len(sent_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a910b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'basque', 'bengali', 'catalan', 'chinese', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hebrew', 'hinglish', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f86b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " {'between', 'not', 'we', 'where', \"isn't\", 'on', 'did', 'when', 'nor', 'now', \"haven't\", 'having', 'and', \"shan't\", 'had', 'they', 'more', 'by', 'y', 'he', 'does', 'itself', 'haven', 'why', 'doesn', 'shan', 'so', \"you've\", 'it', 'them', 'these', 'd', 'you', \"needn't\", 'herself', 'himself', 'her', 'was', 'your', \"should've\", 'isn', 'will', 'under', 'mustn', 'only', 'again', 'over', 's', \"you're\", \"wasn't\", 'wasn', \"didn't\", 'but', 'as', 'ma', 'while', 'up', 'theirs', 'is', 'other', 'hasn', 'should', 'each', 'him', 'in', 'both', 'themselves', 'to', 'very', 'any', 'through', 'do', 'a', 'aren', 'few', 'o', 'his', 'their', 'has', 'most', \"that'll\", 'mightn', 'didn', 'here', \"couldn't\", 'shouldn', 'below', 'll', \"mightn't\", \"wouldn't\", 'there', 'or', 'couldn', 'those', 'ours', 'down', 'during', 'being', 'were', 'won', 'further', 'm', 'ain', 't', 'weren', 'who', 'that', 'hadn', 'above', 'she', 'have', 'which', 'this', 'are', 'with', 'yourselves', 'such', 'of', 'out', \"won't\", 'at', 'about', 'its', \"mustn't\", 'too', \"doesn't\", 'after', 'i', \"it's\", 'our', 'my', \"you'd\", 'same', 'myself', 'for', 'be', 'how', 'than', 'just', \"you'll\", \"she's\", 'if', \"aren't\", \"shouldn't\", \"weren't\", 'whom', 'from', 'am', 'the', 'before', 'own', 'don', \"hadn't\", \"don't\", 'because', 'needn', 'an', 'hers', 'yours', 'once', 'into', 'all', 'then', 'against', 'been', 'ourselves', 're', 've', 'some', \"hasn't\", 'until', 'yourself', 'me', 'what', 'off', 'no', 'can', 'doing', 'wouldn'}\n"
     ]
    }
   ],
   "source": [
    "stopword = set(stopwords.words('english'))\n",
    "print('\\n',stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0bd1568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.', 'Barracck', 'Obamo', 'president', 'USA']\n"
     ]
    }
   ],
   "source": [
    "stopwords_list = []\n",
    "for word in word_tokens:\n",
    "    if word not in  stopword:\n",
    "        stopwords_list.append(word)\n",
    "print('\\n',stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21ae0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming to remove prefix anf postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f3ecc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['narendra', 'modi', 'prime', 'minist', 'india', '.', 'barracck', 'obamo', 'presid', 'usa']\n"
     ]
    }
   ],
   "source": [
    "from nltk import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "stems = []\n",
    "for word in stopwords_list :\n",
    "   stems.append(ps.stem(word))\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a77aeeeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ['Narendra']\n",
      "\n",
      " ['Narendra', 'Modi']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.', 'Barracck']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.', 'Barracck', 'Obamo']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.', 'Barracck', 'Obamo', 'president']\n",
      "\n",
      " ['Narendra', 'Modi', 'Prime', 'Minister', 'India', '.', 'Barracck', 'Obamo', 'president', 'USA']\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "lma = []\n",
    "for word in stopwords_list:\n",
    "    lma.append(lemma.lemmatize(word))\n",
    "    print('\\n',lma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49f30d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Narendra', 'NNP'), ('Modi', 'NNP'), ('123', 'CD'), ('is', 'VBZ'), ('the', 'DT'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('of', 'IN'), ('India', 'NNP'), ('.', '.'), ('Barracck', 'NNP'), ('Obamo', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('USA', 'NNP')]\n",
      "\n",
      " [('Narendra', 'NNP'), ('Modi', 'NNP'), ('123', 'CD'), ('is', 'VBZ'), ('the', 'DT'), ('Prime', 'NNP'), ('Minister', 'NNP'), ('of', 'IN'), ('India', 'NNP'), ('.', '.'), ('Barracck', 'NNP'), ('Obamo', 'NNP'), ('is', 'VBZ'), ('the', 'DT'), ('president', 'NN'), ('of', 'IN'), ('USA', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "text = \"Narendra Modi 123 is the Prime Minister of India. Barracck Obamo is the president of USA\"\n",
    "from nltk import pos_tag,word_tokenize\n",
    "word_tokens = word_tokenize(text)\n",
    "print(pos_tag(word_tokens))\n",
    "print('\\n',pos_tag(word_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ec282ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Sachin/NNP)\n",
      "  (PERSON Tendulkar/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  creackter/NN\n",
      "  located/VBN\n",
      "  in/IN\n",
      "  (GPE India/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize,pos_tag,ne_chunk\n",
    "text = \"Sachin Tendulkar is a creackter located in India.\"\n",
    "token = word_tokenize(text)\n",
    "pos = pos_tag(token)\n",
    "print(ne_chunk(pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b9270",
   "metadata": {},
   "source": [
    "practical 4 regular expresson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d88b3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500']\n",
      "None\n",
      "<re.Match object; span=(7, 16), match='Tendulkar'>\n",
      "Abdul kalam is a great scientist.His remuneration is $500.\n"
     ]
    }
   ],
   "source": [
    "text = \"Sachin Tendulkar is a creackter.His remuneration is $500.\"\n",
    "import re\n",
    "digit = re.findall('\\d+',text)\n",
    "print(digit)\n",
    "match = re.match('^His',text)\n",
    "print(match)\n",
    "search = re.search('Tendulkar',text)\n",
    "print(search)\n",
    "substitute = re.sub(\"Sachin Tendulkar\",\"Abdul kalam\",text)\n",
    "text1 = re.sub(\"creackter\",\"great scientist\",substitute)\n",
    "print(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28f950ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Narendra Ameer khan 123 is the Prime Minister of India. Barracck Obamo is the president of USA\n",
      "Narendra Modi 123 is the Prime Minister . Barracck Obamo is the president of USA\n"
     ]
    }
   ],
   "source": [
    "text = \"Narendra Modi 123 is the Prime Minister of India. Barracck Obamo is the president of USA\"\n",
    "import re\n",
    "print(re.sub(\"Modi\",\"Ameer khan\",text))\n",
    "print(re.sub(\"of India\",\"\",text))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2e79912",
   "metadata": {},
   "source": [
    "# cotext free grammer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c67a1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (Det the) (N boy))\n",
      "  (VP\n",
      "    (V saw)\n",
      "    (NP\n",
      "      (NP (Det the) (N girl))\n",
      "      (PP (P with) (NP (Det the) (N telescope))))))\n"
     ]
    },
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 17\u001b[0m\n\u001b[0;32m      5\u001b[0m grammer \u001b[38;5;241m=\u001b[39m CFG\u001b[38;5;241m.\u001b[39mfromstring(\u001b[38;5;124m'''\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mS -> NP VP\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mNP -> Det N | NP PP\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124mP -> \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124m'''\u001b[39m)\n\u001b[0;32m     16\u001b[0m parser \u001b[38;5;241m=\u001b[39m RecursiveDescentParser(grammer)\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mparse(sentence):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28mprint\u001b[39m(tree)\n\u001b[0;32m     19\u001b[0m     tree\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:126\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a tree, expand it.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree[frontier[\u001b[38;5;241m0\u001b[39m]], Tree):\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:225\u001b[0m, in \u001b[0;36mRecursiveDescentParser._expand\u001b[1;34m(self, remaining_text, tree, frontier, production)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_expand(newtree, new_frontier, production)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(\n\u001b[0;32m    226\u001b[0m     remaining_text, newtree, new_frontier \u001b[38;5;241m+\u001b[39m frontier[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    227\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:126\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a tree, expand it.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree[frontier[\u001b[38;5;241m0\u001b[39m]], Tree):\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:225\u001b[0m, in \u001b[0;36mRecursiveDescentParser._expand\u001b[1;34m(self, remaining_text, tree, frontier, production)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_expand(newtree, new_frontier, production)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(\n\u001b[0;32m    226\u001b[0m     remaining_text, newtree, new_frontier \u001b[38;5;241m+\u001b[39m frontier[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    227\u001b[0m )\n",
      "    \u001b[1;31m[... skipping similar frames: RecursiveDescentParser._expand at line 225 (1 times), RecursiveDescentParser._parse at line 126 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:130\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:168\u001b[0m, in \u001b[0;36mRecursiveDescentParser._match\u001b[1;34m(self, rtext, tree, frontier)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_match(newtree, frontier[\u001b[38;5;241m1\u001b[39m:], rtext[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(rtext[\u001b[38;5;241m1\u001b[39m:], newtree, frontier[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# If it's a non-matching terminal, fail.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n",
      "    \u001b[1;31m[... skipping similar frames: RecursiveDescentParser._expand at line 225 (1 times), RecursiveDescentParser._parse at line 126 (1 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:130\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:168\u001b[0m, in \u001b[0;36mRecursiveDescentParser._match\u001b[1;34m(self, rtext, tree, frontier)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_match(newtree, frontier[\u001b[38;5;241m1\u001b[39m:], rtext[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(rtext[\u001b[38;5;241m1\u001b[39m:], newtree, frontier[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# If it's a non-matching terminal, fail.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n",
      "    \u001b[1;31m[... skipping similar frames: RecursiveDescentParser._parse at line 126 (739 times), RecursiveDescentParser._expand at line 225 (738 times), RecursiveDescentParser._match at line 168 (5 times), RecursiveDescentParser._parse at line 130 (5 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:225\u001b[0m, in \u001b[0;36mRecursiveDescentParser._expand\u001b[1;34m(self, remaining_text, tree, frontier, production)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_expand(newtree, new_frontier, production)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(\n\u001b[0;32m    226\u001b[0m     remaining_text, newtree, new_frontier \u001b[38;5;241m+\u001b[39m frontier[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    227\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:130\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:168\u001b[0m, in \u001b[0;36mRecursiveDescentParser._match\u001b[1;34m(self, rtext, tree, frontier)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_match(newtree, frontier[\u001b[38;5;241m1\u001b[39m:], rtext[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse(rtext[\u001b[38;5;241m1\u001b[39m:], newtree, frontier[\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;66;03m# If it's a non-matching terminal, fail.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:126\u001b[0m, in \u001b[0;36mRecursiveDescentParser._parse\u001b[1;34m(self, remaining_text, tree, frontier)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a tree, expand it.\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree[frontier[\u001b[38;5;241m0\u001b[39m]], Tree):\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand(remaining_text, tree, frontier)\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# If the next element on the frontier is a token, match it.\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match(remaining_text, tree, frontier)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\parse\\recursivedescent.py:218\u001b[0m, in \u001b[0;36mRecursiveDescentParser._expand\u001b[1;34m(self, remaining_text, tree, frontier, production)\u001b[0m\n\u001b[0;32m    216\u001b[0m     newtree \u001b[38;5;241m=\u001b[39m subtree\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 218\u001b[0m     newtree \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    219\u001b[0m     newtree[frontier[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m subtree\n\u001b[0;32m    220\u001b[0m new_frontier \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    221\u001b[0m     frontier[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m (i,) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(production\u001b[38;5;241m.\u001b[39mrhs()))\n\u001b[0;32m    222\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:558\u001b[0m, in \u001b[0;36mTree.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 558\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36mTree.convert\u001b[1;34m(cls, tree)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36mTree.convert\u001b[1;34m(cls, tree)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "    \u001b[1;31m[... skipping similar frames: <listcomp> at line 543 (730 times), Tree.convert at line 543 (730 times)]\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36mTree.convert\u001b[1;34m(cls, tree)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:543\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;124;03mConvert a tree between different subtypes of Tree.  ``cls`` determines\u001b[39;00m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;124;03mwhich class will be used to encode the new tree.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m:return: The new Tree.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[1;32m--> 543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:544\u001b[0m, in \u001b[0;36mTree.convert\u001b[1;34m(cls, tree)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tree, Tree):\n\u001b[0;32m    543\u001b[0m     children \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconvert(child) \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m tree]\n\u001b[1;32m--> 544\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(tree\u001b[38;5;241m.\u001b[39m_label, children)\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tree\\tree.py:103\u001b[0m, in \u001b[0;36mTree.__init__\u001b[1;34m(self, node, children)\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m() argument 2 should be a list, not a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28mlist\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, children)\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_label \u001b[38;5;241m=\u001b[39m node\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import CFG\n",
    "from nltk.parse import RecursiveDescentParser\n",
    "sentence = \"the boy saw the girl with the telescope\".split()\n",
    "grammer = CFG.fromstring('''\n",
    "S -> NP VP\n",
    "NP -> Det N | NP PP\n",
    "VP -> V NP\n",
    "PP -> P NP \n",
    "Det -> 'the'\n",
    "N -> 'boy'|'girl'|'telescope'\n",
    "V -> 'saw'\n",
    "P -> 'with'\n",
    "''')\n",
    "\n",
    "parser = RecursiveDescentParser(grammer)\n",
    "for tree in parser.parse(sentence):\n",
    "    print(tree)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ea36ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "unigram : [('Virst',), ('Kohili',), ('is',), ('playing',), ('cricket',), ('.',)]\n",
      "bigram : [('Virst', 'Kohili'), ('Kohili', 'is'), ('is', 'playing'), ('playing', 'cricket'), ('cricket', '.')]\n",
      "trigram : [('Virst', 'Kohili', 'is'), ('Kohili', 'is', 'playing'), ('is', 'playing', 'cricket'), ('playing', 'cricket', '.')]\n",
      "sixgram : [('Virst', 'Kohili', 'is', 'playing', 'cricket', '.')]\n",
      "sevengram : []\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from nltk import word_tokenize\n",
    "text = \"Virst Kohili is playing cricket.\"\n",
    "token = word_tokenize(text)\n",
    "print(len(token))\n",
    "unigram = list(ngrams(token,1))\n",
    "print(\"unigram :\",unigram)\n",
    "bigram = list(ngrams(token,2))\n",
    "print(\"bigram :\",bigram)\n",
    "trigram = list(ngrams(token,3))\n",
    "print(\"trigram :\",trigram)\n",
    "sixgram = list(ngrams(token,6))\n",
    "print(\"sixgram :\",sixgram)\n",
    "sevengram = list(ngrams(token,7))\n",
    "print(\"sevengram :\",sevengram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5bdc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean absolute error,Mean squard error,Root Mean squard Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d2214a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width         species\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n",
    "iris = pd.read_csv(url, header=None, names=column_names)\n",
    "iris"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231f30d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfc41b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE 0.3058125115827802\n",
      "MSE 0.16003788960177026\n",
      "RMSE 0.4000473591985957\n"
     ]
    }
   ],
   "source": [
    "x = iris[['sepal_length']]\n",
    "y = iris.sepal_width\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "\n",
    "training = lr.fit(x_train,y_train)\n",
    "yprd = lr.predict(x_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "mae = metrics.mean_absolute_error(y_test,yprd)\n",
    "print(\"MAE\",mae)\n",
    "mse = metrics.mean_squared_error(y_test,yprd)\n",
    "print(\"MSE\",mse)\n",
    "import numpy as np\n",
    "rmse = np.sqrt(metrics.mean_squared_error(y_test,yprd))\n",
    "print(\"RMSE\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c922a9e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens:  ['Virat', 'Kohli', 'gave', 'Sachin', 'a', 'bat', '.']\n",
      "\n",
      " POS Tags :  [('Virat', 'NNP'), ('Kohli', 'NNP'), ('gave', 'VBD'), ('Sachin', 'NNP'), ('a', 'DT'), ('bat', 'NN'), ('.', '.')]\n",
      "\n",
      " Named_Entities :  (S\n",
      "  (PERSON Virat/NNP)\n",
      "  (PERSON Kohli/NNP)\n",
      "  gave/VBD\n",
      "  (PERSON Sachin/NNP)\n",
      "  a/DT\n",
      "  bat/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Practical#9: Named Entity Recognition\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "text = \"Virat Kohli gave Sachin a bat.\"\n",
    "\n",
    "token = word_tokenize(text)\n",
    "print(\"Tokens: \", token)\n",
    "\n",
    "pos = pos_tag(token)\n",
    "print(\"\\n POS Tags : \", pos)\n",
    "\n",
    "Named_Entity = ne_chunk(pos)\n",
    "print(\"\\n Named_Entities : \", Named_Entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1b020c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# pip install spacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 3 english lexcons : sm, md, lg\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# python -m spacy download en_core_web_sm\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVirat Kohli gave Sachin a bat.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m srl \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# pip install spacy\n",
    "# 3 english lexcons : sm, md, lg\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "text = \"Virat Kohli gave Sachin a bat.\"\n",
    "srl = spacy.load('en_core_web_sm')\n",
    "doc = srl(text)\n",
    "for token in doc:\n",
    "    print(f\"Token:{token.text}, POS:{token.pos_}, Dependency:{token.dep_}, Verb:{token.head.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4562aae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
